{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Word2Vec_Matrix_Factorization.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import sys\n",
        "sys.path.append(os.path.abspath('drive/MyDrive/'))\n",
        "sys.path.append(os.path.abspath('drive/MyDrive/cc_matrix_50.npy'))\n",
        "sys.path"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gIGrM1bIJgxv",
        "outputId": "fa2051ae-ff63-43dc-d098-447869eb31e9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['',\n",
              " '/content',\n",
              " '/env/python',\n",
              " '/usr/lib/python37.zip',\n",
              " '/usr/lib/python3.7',\n",
              " '/usr/lib/python3.7/lib-dynload',\n",
              " '/usr/local/lib/python3.7/dist-packages',\n",
              " '/usr/lib/python3/dist-packages',\n",
              " '/usr/local/lib/python3.7/dist-packages/IPython/extensions',\n",
              " '/root/.ipython',\n",
              " '/content/drive/MyDrive',\n",
              " '/content/drive/MyDrive/cc_matrix_50.npy']"
            ]
          },
          "metadata": {},
          "execution_count": 1
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from util import get_wikipedia_data,find_analogies\n",
        "from brown import get_sentences_with_word2idx_limit_vocab, get_sentences_with_word2idx\n"
      ],
      "metadata": {
        "id": "bkAT55VNKyJr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# New Section"
      ],
      "metadata": {
        "id": "JCWlARl1Jif1"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PJuruUuGJavV",
        "outputId": "d7495996-a944-4b6c-8541-15bd87bb228e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "max in X: 0.0\n",
            "max in f(X): 0.0\n",
            "max in log(X): 0.0\n",
            "time to build co-occurrence matrix: 0:00:00.403982\n",
            "0\n",
            "9.53045845\n",
            "19.0428352\n",
            "0\n",
            "8.85435772\n",
            "17.6908283\n",
            "0\n",
            "8.21760464\n",
            "16.4175377\n",
            "[<tf.Tensor 'add_1:0' shape=() dtype=float32>, <tf.Tensor 'add_1:0' shape=() dtype=float32>]\n",
            "** concat: True\n",
            "closest match by euclidean distance: formal\n",
            "king - man = formal - woman\n",
            "closest match by cosine distance: formal\n",
            "king - man = formal - woman\n",
            "closest match by euclidean distance: kind\n",
            "france - paris = kind - london\n",
            "closest match by cosine distance: kind\n",
            "france - paris = kind - london\n",
            "closest match by euclidean distance: style\n",
            "france - paris = style - rome\n",
            "closest match by cosine distance: style\n",
            "france - paris = style - rome\n",
            "closest match by euclidean distance: officials\n",
            "paris - france = officials - italy\n",
            "closest match by cosine distance: officials\n",
            "paris - france = officials - italy\n",
            "closest match by euclidean distance: tree\n",
            "france - french = tree - english\n",
            "closest match by cosine distance: tree\n",
            "france - french = tree - english\n",
            "closest match by euclidean distance: foreign\n",
            "japan - japanese = foreign - chinese\n",
            "closest match by cosine distance: foreign\n",
            "japan - japanese = foreign - chinese\n",
            "closest match by euclidean distance: risk\n",
            "japan - japanese = risk - italian\n",
            "closest match by cosine distance: risk\n",
            "japan - japanese = risk - italian\n",
            "closest match by euclidean distance: values\n",
            "japan - japanese = values - australian\n",
            "closest match by cosine distance: dark\n",
            "japan - japanese = dark - australian\n",
            "closest match by euclidean distance: hospital\n",
            "december - november = hospital - june\n",
            "closest match by cosine distance: hospital\n",
            "december - november = hospital - june\n",
            "** concat: False\n",
            "closest match by euclidean distance: condition\n",
            "king - man = condition - woman\n",
            "closest match by cosine distance: condition\n",
            "king - man = condition - woman\n",
            "closest match by euclidean distance: kind\n",
            "france - paris = kind - london\n",
            "closest match by cosine distance: kind\n",
            "france - paris = kind - london\n",
            "closest match by euclidean distance: style\n",
            "france - paris = style - rome\n",
            "closest match by cosine distance: style\n",
            "france - paris = style - rome\n",
            "closest match by euclidean distance: closed\n",
            "paris - france = closed - italy\n",
            "closest match by cosine distance: approach\n",
            "paris - france = approach - italy\n",
            "closest match by euclidean distance: tree\n",
            "france - french = tree - english\n",
            "closest match by cosine distance: forest\n",
            "france - french = forest - english\n",
            "closest match by euclidean distance: causing\n",
            "japan - japanese = causing - chinese\n",
            "closest match by cosine distance: week\n",
            "japan - japanese = week - chinese\n",
            "closest match by euclidean distance: physical\n",
            "japan - japanese = physical - italian\n",
            "closest match by cosine distance: week\n",
            "japan - japanese = week - italian\n",
            "closest match by euclidean distance: 4\n",
            "japan - japanese = 4 - australian\n",
            "closest match by cosine distance: 4\n",
            "japan - japanese = 4 - australian\n",
            "closest match by euclidean distance: center\n",
            "december - november = center - june\n",
            "closest match by cosine distance: parts\n",
            "december - november = parts - june\n"
          ]
        }
      ],
      "source": [
        "\n",
        "\n",
        "import os\n",
        "import json\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from datetime import datetime\n",
        "from sklearn.utils import shuffle\n",
        "#from util import find_analogies\n",
        "\n",
        "from util import get_wikipedia_data\n",
        "\n",
        "\n",
        "\n",
        "class mymodel(tf.keras.Model):\n",
        "    def __init__(self, logX,fX,reg,V,D):\n",
        "        super().__init__()\n",
        "        W = np.random.randn(V, D) / np.sqrt(V + D)\n",
        "        b = np.zeros(V)\n",
        "        U = np.random.randn(V, D) / np.sqrt(V + D)\n",
        "        c = np.zeros(V)\n",
        "        self.tfmu = tf.constant(logX.mean(),dtype=tf.float32)\n",
        "\n",
        "        \n",
        "\n",
        "        # initialize weights, inputs, \n",
        "        self.tfW = tf.Variable(W.astype(np.float32))\n",
        "        self.tfb = tf.Variable(b.reshape(V, 1).astype(np.float32))\n",
        "        self.tfU = tf.Variable(U.astype(np.float32))\n",
        "        self.tfc = tf.Variable(c.reshape(1, V).astype(np.float32))\n",
        "        self.tffX  = tf.constant(fX,dtype=tf.float32)\n",
        "        self.logX = tf.constant(logX,dtype=tf.float32)\n",
        "        self.reg = tf.constant(reg,dtype=tf.float32)\n",
        "        \n",
        "\n",
        "    def call(self,inputs):\n",
        "        self.prediction = tf.matmul(self.tfW, tf.transpose(a=self.tfU)) + self.tfb + self.tfc + self.tfmu\n",
        "        \n",
        "        return  self.prediction\n",
        "\n",
        "    def cost(self):\n",
        "        try:\n",
        "            costs = tf.reduce_sum(self.tffX * (self.prediction-self.logX)*(self.prediction-self.logX))\n",
        "            tf.print(costs)\n",
        "        except Exception:\n",
        "            print(\"train model first\")\n",
        "            return\n",
        "\n",
        "\n",
        "        for p in (self.tfW,self.tfU):\n",
        "            costs += self.reg*tf.reduce_sum(p*p)\n",
        "            tf.print(costs)\n",
        "        return costs\n",
        "        \n",
        "    \n",
        "\n",
        "\n",
        "@tf.function\n",
        "def train_step(inputs):\n",
        "    with tf.GradientTape() as tape:\n",
        "    # training=True is only needed if there are layers with different\n",
        "    # behavior during training versus inference (e.g. Dropout).\n",
        "        predictions = model(inputs)\n",
        "        loss = model.cost()\n",
        "    gradients = tape.gradient(loss, model.trainable_variables)\n",
        "    optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n",
        "    losses.append(loss)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "class Glove:\n",
        "    def __init__(self, D, V, context_sz):\n",
        "        self.D = D\n",
        "        self.V = V\n",
        "        self.context_sz = context_sz\n",
        "\n",
        "    def fit(self, sentences, cc_matrix=None, learning_rate=1e-4, reg=0.1, xmax=100, alpha=0.75, epochs=4):\n",
        "        # build co-occurrence matrix\n",
        "        # paper calls it X, so we will call it X, instead of calling\n",
        "        # the training data X\n",
        "        # TODO: would it be better to use a sparse matrix?\n",
        "        t0 = datetime.now()\n",
        "        V = self.V\n",
        "        D = self.D\n",
        "\n",
        "        if not os.path.exists(cc_matrix):\n",
        "            X = np.zeros((V, V))\n",
        "            N = len(sentences)\n",
        "            print(\"number of sentences to process:\", N)\n",
        "            it = 0\n",
        "            for sentence in sentences:\n",
        "                it += 1\n",
        "                if it % 10000 == 0:\n",
        "                    print(\"processed\", it, \"/\", N)\n",
        "                n = len(sentence)\n",
        "                for i in range(n):\n",
        "                    # i is not the word index!!!\n",
        "                    # j is not the word index!!!\n",
        "                    # i just points to which element of the sequence (sentence) we're looking at\n",
        "                    wi = sentence[i]\n",
        "\n",
        "                    start = max(0, i - self.context_sz)\n",
        "                    end = min(n, i + self.context_sz)\n",
        "\n",
        "                    # we can either choose only one side as context, or both\n",
        "                    # here we are doing both\n",
        "\n",
        "                    # make sure \"start\" and \"end\" tokens are part of some context\n",
        "                    # otherwise their f(X) will be 0 (denominator in bias update)\n",
        "                    if i - self.context_sz < 0:\n",
        "                        points = 1.0 / (i + 1)\n",
        "                        X[wi,0] += points\n",
        "                        X[0,wi] += points\n",
        "                    if i + self.context_sz > n:\n",
        "                        points = 1.0 / (n - i)\n",
        "                        X[wi,1] += points\n",
        "                        X[1,wi] += points\n",
        "\n",
        "                    # left side\n",
        "                    for j in range(start, i):\n",
        "                        wj = sentence[j]\n",
        "                        points = 1.0 / (i - j) # this is +ve\n",
        "                        X[wi,wj] += points\n",
        "                        X[wj,wi] += points\n",
        "\n",
        "                    # right side\n",
        "                    for j in range(i + 1, end):\n",
        "                        wj = sentence[j]\n",
        "                        points = 1.0 / (j - i) # this is +ve\n",
        "                        X[wi,wj] += points\n",
        "                        X[wj,wi] += points\n",
        "\n",
        "            # save the cc matrix because it takes forever to create\n",
        "            np.save(cc_matrix, X)\n",
        "        else:\n",
        "            X = np.load(cc_matrix)\n",
        "\n",
        "        print(\"max in X:\", X.max())\n",
        "\n",
        "        # weighting\n",
        "        fX = np.zeros((V, V))\n",
        "        fX[X < xmax] = (X[X < xmax] / float(xmax)) ** alpha\n",
        "        fX[X >= xmax] = 1\n",
        "\n",
        "        print(\"max in f(X):\", fX.max())\n",
        "\n",
        "        # target\n",
        "        logX = np.log(X + 1)\n",
        "\n",
        "        print(\"max in log(X):\", logX.max())\n",
        "\n",
        "        print(\"time to build co-occurrence matrix:\", (datetime.now() - t0))\n",
        "\n",
        "        # initialize weights\n",
        "        return logX,fX,reg,self.V,self.D\n",
        "\n",
        "\n",
        "\n",
        "def main(we_file, w2i_file, use_brown=True, n_files=50):\n",
        "    if use_brown:\n",
        "        cc_matrix = \"cc_matrix_brown.npy\"\n",
        "    else:\n",
        "        cc_matrix = \"cc_matrix_%s.npy\" % n_files\n",
        "\n",
        "    # hacky way of checking if we need to re-load the raw data or not\n",
        "    # remember, only the co-occurrence matrix is needed for training\n",
        "    if os.path.exists('drive/MyDrive/cc_matrix_50.npy'):\n",
        "        with open(f\"drive/MyDrive/{w2i_file}\") as f:\n",
        "            word2idx = json.load(f)\n",
        "        sentences = [] # dummy - we won't actually use it\n",
        "    else:\n",
        "        if use_brown:\n",
        "            keep_words = set([\n",
        "                'king', 'man', 'woman',\n",
        "                'france', 'paris', 'london', 'rome', 'italy', 'britain', 'england',\n",
        "                'french', 'english', 'japan', 'japanese', 'chinese', 'italian',\n",
        "                'australia', 'australian', 'december', 'november', 'june',\n",
        "                'january', 'february', 'march', 'april', 'may', 'july', 'august',\n",
        "                'september', 'october',\n",
        "            ])\n",
        "            sentences, word2idx = get_sentences_with_word2idx_limit_vocab(n_vocab=5000, keep_words=keep_words)\n",
        "        else:\n",
        "            sentences, word2idx = get_wikipedia_data(n_files=n_files, n_vocab=2000)\n",
        "        \n",
        "        with open(w2i_file, 'w') as f:\n",
        "            json.dump(word2idx, f)\n",
        "\n",
        "    V = len(word2idx)\n",
        "    model = Glove(100, V, 10)\n",
        "    return model.fit(sentences, cc_matrix=cc_matrix, epochs=200)\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    we = 'glove_model_50.npz'\n",
        "    w2i = 'glove_word2idx_50.json'\n",
        "    logX, fX,reg , V, D= main(we, w2i, use_brown=False)\n",
        "\n",
        "    model = mymodel(logX,fX,reg,V,D) \n",
        "    optimizer = tf.keras.optimizers.Adam()\n",
        "    losses = []\n",
        "    for epoch in range(3):\n",
        "        train_step(0)\n",
        "\n",
        "\n",
        "    print(losses)\n",
        "\n",
        "    arrays = [model.tfW.numpy(), tf.transpose(model.tfU).numpy()]\n",
        "    np.savez(we, *arrays)\n",
        "\n",
        "    # load back embeddings\n",
        "    npz = np.load(we)\n",
        "    W1 = npz['arr_0']\n",
        "    W2 = npz['arr_1']\n",
        "\n",
        "    with open(f\"drive/MyDrive/{w2i}\") as f:\n",
        "        word2idx = json.load(f)\n",
        "        idx2word = {i:w for w,i in word2idx.items()}\n",
        "\n",
        "    for concat in (True, False):\n",
        "        print(\"** concat:\", concat)\n",
        "\n",
        "        if concat:\n",
        "            We = np.hstack([W1, W2.T])\n",
        "        else:\n",
        "            We = (W1 + W2.T) / 2\n",
        "\n",
        "\n",
        "        find_analogies('king', 'man', 'woman', We, word2idx, idx2word)\n",
        "        find_analogies('france', 'paris', 'london', We, word2idx, idx2word)\n",
        "        find_analogies('france', 'paris', 'rome', We, word2idx, idx2word)\n",
        "        find_analogies('paris', 'france', 'italy', We, word2idx, idx2word)\n",
        "        find_analogies('france', 'french', 'english', We, word2idx, idx2word)\n",
        "        find_analogies('japan', 'japanese', 'chinese', We, word2idx, idx2word)\n",
        "        find_analogies('japan', 'japanese', 'italian', We, word2idx, idx2word)\n",
        "        find_analogies('japan', 'japanese', 'australian', We, word2idx, idx2word)\n",
        "        find_analogies('december', 'november', 'june', We, word2idx, idx2word)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "od_lFxwSNR_8"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}